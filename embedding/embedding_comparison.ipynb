{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('api_key')\n",
    "endpoint = os.getenv('endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model_1 = 'BAAI/bge-base-en-v1.5'\n",
    "# tokenizer_baai = AutoTokenizer.from_pretrained(embedding_model_1)\n",
    "# model_baai = AutoModel.from_pretrained(embedding_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_embedding_baai(text):\n",
    "#     inputs = tokenizer_baai(text, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "    \n",
    "#     # Generate the embeddings \n",
    "#     with torch.no_grad():    \n",
    "#         embeddings = model_baai(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "#     return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model_bert = BertModel.from_pretrained(\"bert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_embedding(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "    \n",
    "    # Generate the embeddings \n",
    "    with torch.no_grad():    \n",
    "        embeddings = model_bert(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"?api-version=2024-02-01&model-version=2023-04-15\"\n",
    "version_old = \"?api-version=2024-02-01&model-version=2022-04-11\"\n",
    "\n",
    "vec_img_url = endpoint + \"/computervision/retrieval:vectorizeImage\" + version  # For doing the image vectorization\n",
    "vec_txt_url = endpoint + \"/computervision/retrieval:vectorizeText\" + version  # For the prompt vectorization\n",
    "vec_txt_url_old = endpoint + \"/computervision/retrieval:vectorizeText\" + version_old\n",
    "\n",
    "headers = {\n",
    "    'Content-type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_image_embedding(imageurl):\n",
    "    image = {'url': imageurl}\n",
    "    r = requests.post(vec_img_url, data=json.dumps(image), headers=headers)\n",
    "    print(r.json())\n",
    "    image_emb = r.json()['vector']\n",
    "\n",
    "    return image_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_text_embedding(promptxt):\n",
    "    prompt = {'text': promptxt}\n",
    "    r = requests.post(vec_txt_url, data=json.dumps(prompt), headers=headers)\n",
    "    text_emb = r.json()['vector']\n",
    "\n",
    "    return text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_text_embedding_old(promptxt):\n",
    "    prompt = {'text': promptxt}\n",
    "    r = requests.post(vec_txt_url_old, data=json.dumps(prompt), headers=headers)\n",
    "    text_emb = r.json()['vector']\n",
    "\n",
    "    return text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(vector1, vector2):\n",
    "    dot_product = 0\n",
    "    length = min(len(vector1), len(vector2))\n",
    "\n",
    "    for i in range(length):\n",
    "        dot_product += vector1[i] * vector2[i]\n",
    "\n",
    "    magnitude1 = math.sqrt(sum(x * x for x in vector1))\n",
    "    magnitude2 = math.sqrt(sum(x * x for x in vector2))\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_results(image_emb, prompts, model):\n",
    "    if model == \"gpt\":\n",
    "        simil_values_list = [\n",
    "            get_cosine_similarity(image_emb, gpt_text_embedding(prompt))\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    elif model == \"gpt_old\":\n",
    "        simil_values_list = [\n",
    "            get_cosine_similarity(image_emb, gpt_text_embedding_old(prompt))\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    elif model == \"bert\":\n",
    "        simil_values_list = [\n",
    "            get_cosine_similarity(image_emb, bert_text_embedding(prompt))\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        print(\"model name invalid\")\n",
    "        return\n",
    "    sorted_results = sorted(zip(prompts, simil_values_list),\n",
    "                            key=lambda x: x[1],\n",
    "                            reverse=True)\n",
    "\n",
    "    df = pd.DataFrame(columns=['prompt', 'similarity'])\n",
    "    for idx, (prompt, simil_val) in enumerate(sorted_results):\n",
    "        df.loc[idx, 'prompt'] = prompt\n",
    "        df.loc[idx, 'similarity'] = simil_val\n",
    "\n",
    "    df[\"similarity\"] = df.similarity.astype(float)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageurl1 = \"https://github.com/retkowsky/images/blob/master/i4.jpg?raw=true\"\n",
    "image_emb1 = gpt_image_embedding(imageurl1)\n",
    "plt.imshow(Image.open(requests.get(imageurl1, stream=True).raw))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'bird', 'a truck', 'a car', 'a blue car', 'a white car', 'a BMW white car',\n",
    "    'a tesla car', 'a mercedes car', 'a man', 'a ford car', 'traffic', 'advertisement', 'human'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched version of model\n",
    "gpt_df = similarity_results(image_emb1, prompts, \"gpt\")\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "gpt_df.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both gpt but different model versions\n",
    "gpt_df_old = similarity_results(image_emb1, prompts, \"gpt_old\")\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "gpt_df_old.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt for image embedding and bert for text embedding\n",
    "bert_df = similarity_results(image_emb1, prompts, \"bert\")\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "bert_df.style.background_gradient(cmap=cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
