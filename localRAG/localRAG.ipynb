{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./data/\" + \"<Your file name>\" #Change to name of yout file (make sure the file name does not include any space)\n",
    "\n",
    "offset = 0       #The character count from the start of the document\n",
    "page_map = []    #List of turples: (page_num, offset, page_text)\n",
    "\n",
    "print(f\"Extracting text from '{filename}' using PdfReader\")\n",
    "\n",
    "reader = PdfReader(filename)\n",
    "pages = reader.pages\n",
    "for page_num, p in enumerate(pages):\n",
    "    page_text = p.extract_text()\n",
    "    page_map.append((page_num, offset, page_text))\n",
    "    offset += len(page_text)\n",
    "    \n",
    "page_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import base64\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 500\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "\n",
    "\n",
    "def filename_to_id(filename): \n",
    "    filename_ascii = re.sub(\"[^0-9a-zA-Z_-]\", \"_\", filename)\n",
    "    filename_hash = base64.b16encode(filename.encode('utf-8')).decode('ascii')\n",
    "    return f\"file-{filename_ascii}-{filename_hash}\"\n",
    "\n",
    "def split_text(page_map):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \" \"]\n",
    "\n",
    "    def find_page(offset):\n",
    "        l = len(page_map)\n",
    "        for i in range(l - 1):\n",
    "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "                return i\n",
    "        return l - 1\n",
    "\n",
    "    all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        yield (section_text, find_page(start))\n",
    "\n",
    "        last_table_start = section_text.rfind(\"<table\")\n",
    "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
    "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
    "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
    "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
    "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
    "        else:\n",
    "            start = end - SECTION_OVERLAP\n",
    "        \n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], find_page(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = 'BAAI/bge-small-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "model = AutoModel.from_pretrained(embedding_model)\n",
    "tokenizer.save_pretrained(\"model/tokenizer\")\n",
    "model.save_pretrained(\"model/embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./model/tokenizer\") \n",
    "    model = AutoModel.from_pretrained(\"./model/embedding\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "    \n",
    "    # Generate the embeddings \n",
    "    with torch.no_grad():    \n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "file_id = filename_to_id(filename)\n",
    "for i, (content, pagenum) in enumerate(split_text(page_map)):\n",
    "    section = {\n",
    "        \"id\": f\"{file_id}-page-{i}\",\n",
    "        \"content\": content,\n",
    "        \"embedding\": compute_embedding(content),\n",
    "        \"sourcepage\": os.path.splitext(os.path.basename(filename))[0] + f\"-{pagenum}\" + \".pdf\",\n",
    "        \"sourcefile\": filename\n",
    "    }\n",
    "    sections.append(section)\n",
    "# can download to json file or database and get back for retrieval so that no need for re-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the most-matched parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matches(sections, query_str, top_k):\n",
    "    # Get the embedding for the query string\n",
    "    query_str_embedding = np.array(compute_embedding(query_str))\n",
    "    scores = []\n",
    "\n",
    "    # Calculate the cosine similarity between the query embedding and each chunk's embedding\n",
    "    for section in sections:\n",
    "        embedding_array = section['embedding']\n",
    "        norm_query = np.linalg.norm(query_str_embedding)\n",
    "        norm_chunk = np.linalg.norm(embedding_array)\n",
    "        if norm_query == 0 or norm_chunk == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = np.dot(query_str_embedding, embedding_array)/(norm_query * norm_chunk)\n",
    "            item = {\"id\":section['id'],\n",
    "                    \"content\": section[\"content\"],\n",
    "                    \"sourcepage\":section['sourcepage'],\n",
    "                    \"score\": score}\n",
    "        scores.append(item)\n",
    "\n",
    "    #sorted scores\n",
    "    sorted_scores = sorted(scores, key = lambda x: x['score'], reverse=True)[:top_k]\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Who can apply for URFP?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_matches(sections, user_query, 5)\n",
    "context, temp_context = \"\", \"\"\n",
    "for index, result in enumerate(results):\n",
    "    temp_context += result['content']\n",
    "    tokenized = tokenizer(temp_context, return_tensors=\"pt\")\n",
    "    if len(tokenized[\"input_ids\"][0]) < 500:\n",
    "           context = temp_context\n",
    "    else: break\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use llm to shape the answer from retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "\n",
    "def construct_prompt(context, user_query):\n",
    "    system_prompt = f\"\"\"\n",
    "                    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "                    Your job is to understand the request, and answer based on the retrieved context. According to the retrieved context, if you cannot find the answer to the users'query, say you do not know.\n",
    "                    Do not include any special symbols related to programming language such as \\n in your answer.\n",
    "                    \n",
    "                    Here is the retrieved context\n",
    "                    {context}\n",
    "                    \"\"\"\n",
    "    \n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": user_query}]\n",
    "    return prompt\n",
    "\n",
    "def create_response(messages, model):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        # eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.75,\n",
    "        top_p=0.9,\n",
    "        pad_token_id =tokenizer.eos_token_id\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# Usage\n",
    "\n",
    "prompt = construct_prompt(context, user_query)\n",
    "\n",
    "response = create_response(prompt, model)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
